\input{/home/haldean/Documents/latex_template.tex}
\title{Parallaxis: a Simulated Volumetric Display}

\begin{document}
\maketitle

\begin{abstract}
  Parallaxis is a system written in C++ for computers equipped with
  only a webcam to simulate a three-dimensional volumetric display on
  a computer monitor. It simulates this display by tracking the user's
  head in successive frames, analyzing its motion, and then
  manipulating a displayed scene to reflect the new position of the
  head.
\end{abstract}

\tableofcontents
\newpage

\section{Background Information}
\subsection{Volumetric Displays}
Volumetric displays (devices capable of three-dimensional video
output) have long been relegated to the realm of science fiction;
until recently, volumetric displays existed only in imaginations. In
late 2009, Sony unveiled the world's first feasible volumetric display
at the DC Expo in Tokyo. This device was capable
of a resolution of 98 by 98 by 128-pixel resolution, and the
cylindrical display was 13 centimeters in diameter. Today, two and a
half years later, consumers are still waiting for volumetric displays
to come to market, and while we are seeing a rise in 3D-television
adoption, there seem to be very little demand for holographic or other
kinds of volumetric displays.

On the other hand, simulated volumetric displays (or ``parallax
displays'') like the one I plan on building have seen quite a few
recent and not-so-recent implementations, with various success. Some
rely on specialized hardware, such as Johnny Lee's implementation
which uses a Wiimote as a camera and infrared LEDs mounted on glasses
to determine the user's location in 3D space. Other
systems, mostly for using head motions as input to videogames, use
tracking markers that are worn on a baseball cap. I was unable to find
a popular implementation that did not utilize tracking markers, but
there are published papers that suggest that it has been done, and
with good results. 

\subsection{Motivation}
I wanted to create a volumetric display because it has interesting,
practical uses and is relatively simple to implement, given the
libraries available. It also is something that is fun for an end-user
to use, and easy to integrate into other software. I've found the
available software either heavily platform-dependent (usually
dependent on a Windows system), closed source, or difficult to use for
anything other than very basic, demonstrative purposes. If this
project goes well, I would like to turn it into a cross-platform
library for building interactive parallax display into other 3D
software after the end of the semester.

There are many possible use cases, but I thought of this project idea
when working on my 3D printer. I spend lots of my time modelling
objects for 3D printing, and having the ability to really ``look'' at
an object on a convincing, three-dimensional display before it is
printed would have saved me many reprints. Since I have created the
system, I have tried using it for this purpose with some
success. While it does give you a better feel for the actual shape of
the object, it is limited by its fixed viewpoint; in the current
system, there is no way to look at the back of the object. In future
versions, I would like to either add gestural control so that the user
can spin the device with hand motions, or, if that was intractable, to
have the object itself spin continuously so that the user could see
all sides of the scene.

\section{System Architecture}
The system is split into two separate subsystems; the head detection
subsystem, which polls the camera for frames, detects the head and
returns the relative location, and the graphics subsystem, which is
responsible for turning those relative locations to locations within
the scene at which the camera is placed. As the two subsystems are
completely decoupled, I will discuss the design and implementation of
each, then describe how they interact.

\subsection{Head Detection}
Parallaxis uses the OpenCV C++ library to track heads in the frame. In
my proposal, I outlined two different methods for head tracking that I
could try; in the feedback I received on my proposal, it was suggested
that I try one -- a motion algorithm, as the other algorithm proposed
was similar to the algorithm from homework one. In fact, I ended up
implementing both of these algorithms before deciding to use a third,
that I did not bring up in the proposal. The first two algorithms I
tried I dismissed, one for stability reasons and one for environmental
sensitivity; the third was more accurate than both and extremely
robust, so it was a logical choice.

\subsubsection{Tested Algorithms}
The first algorithm I wrote was the motion-based algorithm. The
results I acheieved from this algorithm were mediocre; while there was
very low latency between the user moving and movement detection, the
results were extremely poor. There was a huge amount of jitter (which
is one of the two measurements upon which I am basing the success of
the system), and attempts to smooth the results resulted in either a
severe lag in motion response or a loss of precision in motion
tracking. 

The second algorithm I tried was a color-based method. The results I
acheived were very dependent upon lighting conditions and other
environmental factors; for example, it worked fairly reliably when I
coded it in my bedroom against a white background, but when I brought
it into my living room, it failed because the walls are cream colored,
which was detected as skin. It also did a poor job in certain lighting
conditions; I have a blue light in my room, and when the blue light
was on it would fail to detect faces. Since I had the goal of making
this a widely-usable library, I decided that this environmental
sensitivity wasn't acceptable.

Since my two proposed methods had turned out to be unreliable, I went
to work researching other methods of facial detection. The industry
standard seems to be the Haar classifier cascade, a series of feature
matching tests done successively, each of which narrows the possible
region in the image where the feature could be. The OpenCV library,
which I was using, contains an implementation of the Haar classifier
cascade, along with a cascade designed specifically for detecting
frontal faces. I used this to implement my own facial detector.

\subsubsection{Subsystem Architecture}
\imgfig{facearch}{0.6}{Architecture of the face detection
  subsystem}{facearch}

The face detector's architecture is shown in \figref{facearch}. The
system polls the camera for a frame. The frame it receives in return
is a raw image in Ipl format (which is OpenCV native format) which is
in full color and is 640 pixels by 480 pixels. This image is copied
(for display later) and then downsampled by a factor of five and
converted to grayscale. This grayscale 128-by-96 image is then fed
into the Haar classifier cascade. The classifier returns a series of
rectangles which define the estimated bounds of the face. The facial
location estimator finds the centroids and areas of each of these
rectangles. It then examines the normalized difference between the
last decision it made (i.e., where it decided the face was in the last
frame) and all possible decisions for this frame, and takes the
rectangle that minimizes this difference to be the current location of
the face. This is important, as the cascade will often detect faces
where there are none; however, usually these can be culled by just
looking at the relative area of the rectangles, as the false positives
are usually either very large or very small. Once it has found the
probable face location, it emits the location of the face and updates
the live monitor display.

This system is not without its drawbacks. I noticed that it was mildly
sensitive to lighting conditions, although it only becomes a real
issue in degenerate lighting conditions where the user is backlit and
therefore in silhouette. It also, interestingly enough, was sensitive
to my glasses. I have heavily-framed glasses, and occasionally it
would lose track on my face with my glasses on. This was especially
true when the lighting was less than optimal; when lighting was good,
the glasses didn't seem to make much of a difference. When lighting
was poor, though, taking off my glasses would often result in a
perfect track where tracking with glasses on was spotty at best.

\subsubsection{Smoothing Methods}
While the classifier method resulted in fairly stable detection, there
was still noise in the detection results. I got around this by using a
standard smoothing algorithm called exponential smoothing. It treats
the output of the face estimator as a linear recurrence, and defines a
second first-order linear recurrence in terms of the previous smoothed
value and current face value. The exact recurrence is as follows
\begin{align*}
  s_1 &= x_0 \\
  s_{t+1} &= (1 - \alpha) x_t + \alpha s_t, \; t \ge 1
\end{align*}

Where $s_n$ is the nth smoothed face value, $x_n$ is the nth detected
face value, and $\alpha$ is a smoothing factor. Higher values of
$\alpha$ mean that the system responds to movement slower, but the
output is smoother, while low $\alpha$ means low latency but higher
jitter. I found that I wanted different smoothing values on the
different axes, because the classifier was better at detecting along
certain axes. More specifically, it did an better job of reliably
determining the face's location along the image's y-axis, but a very
poor job of estimating the size of the face (which I used to determine
the z-component of the face's location). In the end, I used $\alpha_x
= 0.65, \; \alpha_y = 0.6, \; \alpha_z = 0.8$, which resulted in a
fairly accurate and low-latency response to movement, with very little
jitter.

\subsection{Graphics Output}

The graphics output uses the Panda3D game engine to provide a realtime
3D graphics experience. I chose Panda3D because it had bindings for
both C++ and Python, and at the time I wasn't sure which I would
use. It also is available for Mac, Windows and Linux, which meant that
it fulfilled my cross-platform requirement.

The graphics subsystem is significantly more simple than the head
detection subsystem. An overview of the architecture is shown in
\figref{grapharch}. The graphics event loop, which runs at a fixed
rate, is what starts the process of getting a face location and
translating that to a camera location in the scene. I set the event
loop up to run at 10 Hz, as the face detection took an amortized 50
milliseconds and I wanted to pad that a bit in case it took longer for
some reason. 

The events loop first requests the location of the face from the the
head detection system. The returned value has three components:
\code{x}, \code{y} and \code{area}. \code{x} and \code{y} are in the
range $(-0.5, 0.5)$; these values represent the distance between the
center of the face and the center of the image, normalized over the
dimensions of the image. The equation for \code{x} is shown in
\eqnref{xdist}, and the equation for \code{y} is equivalent. There is
also a scaling factor, $s_x$, which determines the response to a
movement along the x-axis. I experimented with these values to
determine the best response along each axis for the highest realism. I
eventually settled on $s_x = 15$ and $s_y = 10$.

The \code{area} component of the face location represents the
estimated area of the face. To translate this to an approximate depth,
I use the equation shown in \eqnref{zdist}. I determined the maximum
face area is the height of the image squared (around 10,000) as this
represents the largest face that the system can
find. $z_{\mathrm{max}}$ represents the farthest away the camera can
get from the model; as the user moves closer to the camera, the area
increases and the camera approaches $z = 0$.

Once the camera location vector has been determined, the camera is set
to the new location and Panda3D takes care of smoothly translating the
current location of the camera to the new location of the camera. This
translation is done so that it aligns with the events loop; since my
events loop runs every tenth of a second, the camera will be moved at
a velocity that ensures that it is at the correct location one-tenth
of a second later.

\begin{equation}
  \label{eq:xdist}
  x = s_x \frac{x_{\mathrm{center}} - x_{\mathrm{face}}}{width}
\end{equation}

\begin{equation}
  \label{eq:zdist}
  z = z_{\mathrm{max}} - \frac{area}{area_{\mathrm{max}}}
\end{equation}

\imgfig{grapharch}{0.6}{Architecture of the graphics
  subsystem}{grapharch}

\section{Execution}

\subsection{Compiling}
To compile Parallaxis, the following libraries and tools are required:
\begin{itemize}
\item OpenCV, version 2.1 or later. This is used for face detection
  and image processing. \url{http://opencv.willowgarage.com}
\item Panda3D, version 1.7 or later. This is used as the 3D engine for
  the graphic display. \url{http://panda3d.org}
\item A C++ compiler.
\item GNU Make
\end{itemize}

To compile Parallaxis, run \code{make}.

\subsection{Usage}
To run Parallaxis, run \code{./parallaxis [model]}, replacing
\code{[model]} with the model you would like to be placed in the
scene. Models must be in EGG format. Models can either be placed in
the models directory, in which case they can be referenced only by
name (for example, the included model \code{cube.egg} in the models
directory can be viewed by calling \code{./parallaxis cube}), or they
must be referenced by an absolute file path.

\section{Results}

As mentioned earlier, the results I got were both believable and had a
fairly low amount of jitter. To analyze output jitter, I plotted the
discrete derivative of the camera location. This plot is shown in
\figref{jitter}. 

\section{Code Listing}
\lstinputlisting[caption={parallax.h}]{../parallax.h}
\lstinputlisting[caption={parallax.cpp}]{../parallax.cpp}
\lstinputlisting[caption={Makefile}]{../Makefile}

\end{document}
